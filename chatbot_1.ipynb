{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gs7rEvyI90H4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "json_string = \"\"\"\n",
    "{\n",
    "  \"intents\": [\n",
    "  {\"tag\": \"greetings\",\n",
    "   \"patterns\": [\"Hii\", \"Hey\", \"Hello\", \"What's going on\", \"Whats's up\", \"How you doing?\"],\n",
    "   \"responses\": [\"Hey!\", \"How are you\", \"What can i do for you\", \"Hello\"]\n",
    "   },\n",
    "  {\n",
    "    \"tag\": \"goodbye\",\n",
    "    \"patterns\":[\"see you later\",\"goodbye\",\"I am leaving\",\"bye\",\"tata\",\"ciao\",\"see ya\"],\n",
    "    \"responses\":[\"Goodbye!\",\"Cao\",\"Sayōnara\",\"See you later\"]\n",
    "  },\n",
    "  {\n",
    "    \"tag\": \"Name\",\n",
    "    \"patterns\":[\"Name\",\"What is your name?\",\"what should i call you?\",\"Apka naam kya hai\",\"cuál es tu nombre\"],\n",
    "    \"responses\":[\"My name is R\",\"U can call me R\"]\n",
    "  },\n",
    "  {\n",
    "    \"tag\": \"slang\",\n",
    "    \"patterns\":[\"fuck\",\"fucker\",\"chutiya\",\"Harami\"],\n",
    "    \"responses\":[\"tu chutiya\",\"tu madarchod\",\"Go FUCK YOUR SELF SOME WHERE ELSE\"]\n",
    " },\n",
    " {\n",
    "    \"tag\": \"language\",\n",
    "    \"patterns\" :[\"What language do you speak?\",\"language\",\"What do u speak?\"],\n",
    "    \"responses\":[\"I speak English\",\"I loved to speak English\",\"English\",\"Some time I like to speak Spanish\"]\n",
    " },\n",
    "   {\"tag\": \"age\",\n",
    "    \"patterns\": [\"How old are you\", \"What is your age\", \"Age?\"],\n",
    "    \"responses\": [\"I am 9!\", \"9 years old\"]  \n",
    "   },\n",
    "\n",
    "   {\"tag\": \"work\",\n",
    "   \"patterns\": [\"What do you do\", \"What you like\", \"What give you happiness\"],\n",
    "   \"responses\": [\"I'll work for you\", \"I would like to help you\", \"By helping you\"]\n",
    "   }\n",
    "\n",
    "]}\"\"\"\n",
    "data = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDKgO-LgLjoH",
    "outputId": "a681ee93-b062-4388-88c7-27f875bce863"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.2.0 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.8.0rc0, 2.8.0rc1, 2.8.0)\n",
      "ERROR: No matching distribution found for tensorflow==1.2.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18504/937604564.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' pip install tensorflow==1.2.0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "! pip install tensorflow==1.2.0\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5dmtGrWMLen"
   },
   "source": [
    "Python pickle module is used for serializing and de-serializing a Python object structure.\n",
    "\n",
    "NLTK (Natural Language Toolkit) Library is a suite that contains libraries and programs for statistical language processing\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
    "\n",
    "Punkt This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model\n",
    "\n",
    "Tensor It is a foundation library that can be used to create Deep Learning models directly or by using wrapper libraries that simplify the process built on top of TensorFlow.\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4lqE1t8PPnu"
   },
   "outputs": [],
   "source": [
    "  lemmatizer = WordNetLemmatizer()\n",
    "  intents = data\n",
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "ignore_letters=['?','!','.',',']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZCP_bHnrT55"
   },
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "  for pattern in intent['patterns']:\n",
    "      word_list=nltk.word_tokenize(pattern)\n",
    "      words.extend(word_list)\n",
    "      documents.append((word_list, intent['tag']))\n",
    "      if intent['tag'] not in classes:\n",
    "          classes.append(intent['tag'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0DypM7Kf-ZY",
    "outputId": "cd8f57e0-75aa-40a2-838d-f6d8bb566caa"
   },
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "\n",
    "pickle.dump(words, open('words.pkl' , 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl' , 'wb'))\n",
    "\n",
    "training = []\n",
    "output_empty = [0]* len(classes)\n",
    "\n",
    "for document in documents:\n",
    "  bag = []\n",
    "  word_patterns = document[0]\n",
    "  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "  for word in words:\n",
    "    bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) \n",
    "hist=model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbotmodel.h5', hist)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up74UsTH4-AG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents =data\n",
    "\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "model = load_model('chatbotmodel.h5')\n",
    "\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "  sentence_words = nltk.word_tokenize(sentence)\n",
    "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "  return sentence_words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "  sentence_words = clean_up_sentence(sentence)\n",
    "  bag = [0] * len(words)\n",
    "  for w in sentence_words:\n",
    "    for i,word in enumerate(words):\n",
    "      if word == w:\n",
    "        bag[i] = 1\n",
    "  return np.array(bag)\n",
    "\n",
    "\n",
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = model.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results  =[[i, r] for i, r in enumerate(res) if r> ERROR_THRESHOLD]\n",
    "\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list=[]\n",
    "    for r in results:\n",
    "        return_list.append({'intent': classes[r[0]],'probability':str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def get_responses(intents_list,intents_json):\n",
    "   tag=intents_list[0]['intent']\n",
    "   list_of_intents=intents_json['intents']\n",
    "   for i in list_of_intents:\n",
    "       if i in list_of_intents:\n",
    "          if i['tag']==tag:\n",
    "              result=random.choice(i['responses'])\n",
    "              break\n",
    "   return result\n",
    "\n",
    "print(\"GO! BOT is running!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2HjZ8TOq5IwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhtg\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "chat=Tk()\n",
    "chat.title(\"Chat_Bot\")\n",
    "chat.geometry(\"400x600\")\n",
    "#function\n",
    "def pri():\n",
    "    ic=str(store.get())\n",
    "    message = ic\n",
    "    ints = predict_class(message)\n",
    "    res= get_responses(ints,intents)\n",
    "    print(res)\n",
    "#head\n",
    "head=Label(chat,text=\"WELCOME TO THE R CHAT BOT\",font=(\"Algerian\",18,\"bold\"))\n",
    "head.pack(side=TOP)\n",
    "#button\n",
    "frame=Frame(chat,bg=\"gray\")\n",
    "frame.pack(side=\"bottom\",anchor=\"se\")\n",
    "but=Button(frame,text=\"Sent\",command=pri)\n",
    "but.pack(anchor=\"se\",side=\"bottom\")\n",
    "#chat bar\n",
    "storage=StringVar()\n",
    "\n",
    "store=Entry(chat, textvariable=storage)\n",
    "store.pack(anchor=\"sw\",side=\"bottom\")\n",
    "\n",
    "\n",
    "chat.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "LqjB_rT16d2I",
    "outputId": "cf88d5ef-6cbf-439c-e71d-2423fc00662e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
